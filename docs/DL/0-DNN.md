# Á•ûÁªèÁΩëÁªúÔºö
Â§öÂ±ÇÊÑüÁü•Êú∫Ôºàmulti-layer perceptron, MLPÔºâ‰πüÂè´‰ΩúÊ∑±Â∫¶ÂâçÈ¶àÁΩëÁªúÔºàdeep feedforward networkÔºâÊàñÂâçÈ¶àÁ•ûÁªèÁΩëÁªúÔºàfeedforward neural networkÔºâÔºå ÂÆÉÈÄöËøáÂ∑≤ÊúâÁöÑ‰ø°ÊÅØÊàñËÄÖÁü•ËØÜÊù•ÂØπÊú™Áü•‰∫ãÁâ©ËøõË°åÈ¢ÑÊµãÔºé
## 1„ÄÅDNN

## ‰∫å„ÄÅÂü∫‰∫éÊ¢ØÂ∫¶ÁöÑÂ≠¶‰π†
### Ôºà1Ôºâ‰ª£‰ª∑ÂáΩÊï∞

### Ôºà2ÔºâËæìÂá∫ÂçïÂÖÉÂΩ¢Âºè

‚ùë $o ‚àà R^d$ ËæìÂá∫Â±û‰∫éÊï¥‰∏™ÂÆûÊï∞Á©∫Èó¥,ÊàñËÄÖÊüêÊÆµÊôÆÈÄöÁöÑÂÆûÊï∞Á©∫Èó¥,ÊØîÂ¶ÇÂáΩÊï∞ÂÄºË∂ãÂäøÁöÑÈ¢ÑÊµã,Âπ¥ÈæÑÁöÑÈ¢ÑÊµãÈóÆÈ¢òÁ≠â„ÄÇ**ËæìÂá∫Â±ÇÁõ¥Êé•ÊÅíÁ≠âÊò†Â∞Ñ**ÔºàÂèØ‰ª•‰∏çÂä†ÊøÄÊ¥ªÂáΩÊï∞Ôºâ„ÄÇËØØÂ∑ÆÁöÑËÆ°ÁÆóÁõ¥Êé•Âü∫‰∫éÊúÄÂêé‰∏ÄÂ±ÇÁöÑËæìÂá∫oÂíåÁúüÂÆûÂÄº y ËøõË°åËÆ°ÁÆó,Â¶ÇÈááÁî®ÂùáÊñπÂ∑ÆËØØÂ∑ÆÂáΩÊï∞Â∫¶ÈáèËæìÂá∫ÂÄºo‰∏éÁúüÂÆûÂÄºy‰πãÈó¥ÁöÑË∑ùÁ¶ª:  
‚ùë $o ‚àà [0,1]$ ËæìÂá∫ÂÄºÁâπÂà´Âú∞ËêΩÂú®[0, 1]ÁöÑÂå∫Èó¥,Â¶ÇÂõæÁâáÁîüÊàê,ÂõæÁâáÂÉèÁ¥†ÂÄº‰∏ÄËà¨Áî®[0, 1]Ë°®Á§∫;ÊàñËÄÖ‰∫åÂàÜÁ±ªÈóÆÈ¢òÁöÑÊ¶ÇÁéá,Â¶ÇÁ°¨Â∏ÅÊ≠£ÂèçÈù¢ÁöÑÊ¶ÇÁéáÈ¢ÑÊµãÈóÆÈ¢ò**SigmoidÔºÅÔºÅ**  
‚ùë $o ‚àà [0, 1]$, ËæìÂá∫ÂÄºËêΩÂú®[0, 1]ÁöÑÂå∫Èó¥,Âπ∂‰∏îÊâÄÊúâËæìÂá∫ÂÄº‰πãÂíå‰∏∫ 1,Â∏∏ËßÅÁöÑÂ¶ÇÂ§öÂàÜÁ±ªÈóÆÈ¢ò,Â¶Ç MNIST ÊâãÂÜôÊï∞Â≠óÂõæÁâáËØÜÂà´,ÂõæÁâáÂ±û‰∫é 10 ‰∏™Á±ªÂà´ÁöÑÊ¶ÇÁéá‰πãÂíå‰∏∫ 1„ÄÇ**SoftmaxÔºÅÔºÅ**  
‚ùë $o ‚àà [‚àí1, 1]$ ËæìÂá∫ÂÄºÂú®[-1, 1]‰πãÈó¥„ÄÇ**tanhÔºÅÔºÅ**

## ‰∏â„ÄÅÈöêËóèÂ±ÇÔºåÈöêËóèÂçïÂÖÉ
### Ôºà1ÔºâÊøÄÊ¥ªÂáΩÊï∞Ôºö

#### 1 sigmoid/logistic
[sigmoid Âíå softmax ÂáΩÊï∞Âå∫Âà´](https://zhuanlan.zhihu.com/p/35697684)

![[Pasted image 20231026180844.png|300]]


- ÂΩì‰∫∫‰ª¨ÈÄêÊ∏êÂÖ≥Ê≥®Âà∞Âà∞Âü∫‰∫éÊ¢ØÂ∫¶ÁöÑÂ≠¶‰π†Êó∂Ôºå sigmoidÂáΩÊï∞ÊòØ‰∏Ä‰∏™Ëá™ÁÑ∂ÁöÑÈÄâÊã©ÔºåÂõ†‰∏∫ÂÆÉÊòØ‰∏Ä‰∏™Âπ≥ÊªëÁöÑ„ÄÅÂèØÂæÆÁöÑÈòàÂÄºÂçïÂÖÉËøë‰ºº„ÄÇ ÂΩìÊàë‰ª¨ÊÉ≥Ë¶ÅÂ∞ÜËæìÂá∫ËßÜ‰Ωú‰∫åÂÖÉÂàÜÁ±ªÈóÆÈ¢òÁöÑÊ¶ÇÁéáÊó∂Ôºå sigmoid‰ªçÁÑ∂Ë¢´ÂπøÊ≥õÁî®‰ΩúËæìÂá∫ÂçïÂÖÉ‰∏äÁöÑÊøÄÊ¥ªÂáΩÊï∞ ÔºàsigmoidÂèØ‰ª•ËßÜ‰∏∫softmaxÁöÑÁâπ‰æãÔºâ
![[Pasted image 20230602020620.png]]
- ÂØπ‰∫é‰∏Ä‰∏™ÂÆö‰πâÂüüÂú®R‰∏≠ÁöÑËæìÂÖ•Ôºå¬†sigmoidÂáΩÊï∞Â∞ÜËæìÂÖ•ÂèòÊç¢‰∏∫Âå∫Èó¥(0, 1)‰∏äÁöÑËæìÂá∫„ÄÇ Âõ†Ê≠§ÔºåsigmoidÈÄöÂ∏∏Áß∞‰∏∫_Êå§ÂéãÂáΩÊï∞_Ôºàsquashing functionÔºâÔºö ÂÆÉÂ∞ÜËåÉÂõ¥Ôºà-inf, infÔºâ‰∏≠ÁöÑ‰ªªÊÑèËæìÂÖ•ÂéãÁº©Âà∞Âå∫Èó¥Ôºà0, 1Ôºâ‰∏≠ÁöÑÊüê‰∏™ÂÄºÔºö
$$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$$
- sigmoidÂáΩÊï∞ÁöÑÂØºÊï∞Ôºö**ÂΩìËæìÂÖ•‰∏∫0Êó∂ÔºåsigmoidÂáΩÊï∞ÁöÑÂØºÊï∞ËææÂà∞ÊúÄÂ§ßÂÄº0.25**Ôºõ ËÄåËæìÂÖ•Âú®‰ªª‰∏ÄÊñπÂêë‰∏äË∂äËøúÁ¶ª0ÁÇπÊó∂ÔºåÂØºÊï∞Ë∂äÊé•Ëøë0„ÄÇ$$\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right).$$
![[Pasted image 20231026165525.png|500]]

**Sigmoid's limitationsÔºö**
- Âú®Ôºà-3Ôºå 3Ôºâ‰ª•Â§ñÂå∫ÂüüÔºåÂØºÊï∞Ë∂ãËøë‰∫é0ÔºåÊ¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢ò„ÄÇÔºà_Vanishing gradient_¬†problemÔºâ
- The output of the logistic function is not symmetric around zero. So the output of all the neurons will be of the same sign. This makes the¬†[training of the neural network](https://www.v7labs.com/training)¬†more difficult and unstable

#### 2 tanhÂáΩÊï∞Ôºö
- ‰∏ésigmoidÂáΩÊï∞Á±ª‰ººÔºå tanh(ÂèåÊõ≤Ê≠£Âàá)ÂáΩÊï∞‰πüËÉΩÂ∞ÜÂÖ∂ËæìÂÖ•ÂéãÁº©ËΩ¨Êç¢Âà∞Âå∫Èó¥(-1, 1)‰∏ä„ÄÇ tanhÂáΩÊï∞ÁöÑÂÖ¨ÂºèÂ¶Ç‰∏ãÔºö$$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.$$
- tanhÂáΩÊï∞ÂØºÊï∞Ôºö$$\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).$$**ÂΩìËæìÂÖ•Êé•Ëøë0Êó∂ÔºåtanhÂáΩÊï∞ÁöÑÂØºÊï∞Êé•ËøëÊúÄÂ§ßÂÄº1**„ÄÇ ‰∏éÊàë‰ª¨Âú®sigmoidÂáΩÊï∞ÂõæÂÉè‰∏≠ÁúãÂà∞ÁöÑÁ±ª‰ººÔºå ËæìÂÖ•Âú®‰ªª‰∏ÄÊñπÂêë‰∏äË∂äËøúÁ¶ª0ÁÇπÔºåÂØºÊï∞Ë∂äÊé•Ëøë0

![[Pasted image 20230602021036.png]]

#### 3 RELU

_‰øÆÊ≠£Á∫øÊÄßÂçïÂÖÉ_ÔºàRectified linear unitÔºå_ReLU_Ôºâ
$$\operatorname{ReLU}(x) = \max(x, 0).$$
Ê±ÇÂØºË°®Áé∞ÂæóÁâπÂà´Â•ΩÔºö**Ë¶Å‰πàËÆ©ÂèÇÊï∞Ê∂àÂ§±ÔºåË¶Å‰πàËÆ©ÂèÇÊï∞ÈÄöËøá**„ÄÇ Ëøô‰ΩøÂæó‰ºòÂåñË°®Áé∞ÂæóÊõ¥Â•ΩÔºåÂπ∂‰∏îReLUÂáèËΩª‰∫ÜÂõ∞Êâ∞‰ª•ÂæÄÁ•ûÁªèÁΩëÁªúÁöÑÊ¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢ò„ÄÇ
![[Pasted image 20230602020639.png]]

  **‰∏âËÄÖÁöÑ‰∏ªË¶ÅÂå∫Âà´**Ôºö 

- Sigmoid ÂûãÊøÄÊ¥ªÂáΩÊï∞‰ºöÂØºËá¥‰∏Ä‰∏™ÈùûÁ®ÄÁñèÁöÑÁ•ûÁªèÁΩëÁªúÔºåËÄå **ReLU Âç¥ÂÖ∑ÊúâÂæàÂ•Ω ÁöÑÁ®ÄÁñèÊÄß**ÔºåÂ§ßÁ∫¶50%ÁöÑÁ•ûÁªèÂÖÉ‰ºöÂ§Ñ‰∫éÊøÄÊ¥ªÁä∂ÊÄÅÔºé 
- Âú®‰ºòÂåñÊñπÈù¢ÔºåÁõ∏ÊØî‰∫éSigmoidÂûãÂáΩÊï∞ÁöÑ‰∏§Á´ØÈ•±ÂíåÔºåReLUÂáΩÊï∞‰∏∫Â∑¶È•±ÂíåÂáΩÊï∞Ôºå **‰∏îÂú® ùë• > 0 Êó∂ÂØºÊï∞‰∏∫ 1ÔºåÂú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äÁºìËß£‰∫ÜÁ•ûÁªèÁΩëÁªúÁöÑÊ¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢ò**ÔºåÂä†ÈÄüÊ¢ØÂ∫¶‰∏ãÈôçÁöÑÊî∂ÊïõÈÄüÂ∫¶Ôºé

>  **üí°Note:**¬†¬†Although both sigmoid and tanh face vanishing gradient issue, _tanh is zero centered, and the gradients are not restricted to move in a certain direction._ Therefore, in practice, tanh nonlinearity is always preferred to sigmoid nonlinearity.
#### 4 others

| Active function | Mathematical fomula    | advantage | limitations |
| --------------- | ---------------------- | --------- | ----------- |
| Leaky ReLU      | $f(x) = max(0.1x,x)$              |           |             |
| Parametric ReLU | $f(x) = max(ax,x)$                                                                                                                             |           |             |
| ELU             | $\left\{\begin{array}{ll} x & \text { for } x  \geqslant 0 \\ \alpha\left(e^{x}-1\right) & \text {for } x<0\end{array}\right.$            |           |             |
| **GELU**        | $\begin{array}{c}f(x)=x P(X \leq x)=x \Phi(x) \\=0.5 x\left(1+\tanh \left[\sqrt{2 / \pi}\left(x+0.044715x^{3}\right)\right]\right)\end{array}$ |           |             |
| SELU            | ![[Pasted image 20231026181024.png\|300]]              |           |             |
| **Swish**       | $f(x) = x \times sigmoid(x)$                      |           |             |






![[Pasted image 20231026174818.png]]



#### 5 how to choose
Here‚Äôs what you should keep in mind.  
As a rule of thumb, you can begin with using the ReLU activation function and then move over to other activation functions if ReLU doesn‚Äôt provide optimum results.

>And here are a few other guidelines to help you out.
1. ReLU activation function should only be used in the hidden layers.
2. Sigmoid/Logistic and Tanh functions should not be used in hidden layers as they make the model more susceptible to problems during training (due to vanishing gradients).
3. Swish function is used in neural networks having a depth greater than 40 layers.  

>Finally, a few rules for choosing the activation function for your output layer based on the type of prediction problem that you are solving:  
1. **Regression**¬†- Linear Activation Function
2. **Binary Classification**‚ÄîSigmoid/Logistic Activation Function
3. **Multiclass Classification**‚ÄîSoftmax
4. **Multilabel Classification**‚ÄîSigmoid  
The activation function used in hidden layers is typically chosen based on the type of neural network architecture.  
5. **Convolutional Neural Network (CNN)**: ReLU activation function.
6. [**Recurrent Neural Network**](https://www.v7labs.com/blog/recurrent-neural-networks-guide): Tanh and/or Sigmoid activation function.


[Activation Functions in Neural Networks](https://www.v7labs.com/blog/neural-networks-activation-functions)
[Activation Functions in Neural Networks--SAGAR SHARMA](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)

[Activation Functions ‚Äî All You Need To Know](https://medium.com/analytics-vidhya/activation-functions-all-you-need-to-know-355a850d025e#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImEwNmFmMGI2OGEyMTE5ZDY5MmNhYzRhYmY0MTVmZjM3ODgxMzZmNjUiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDQwMDU1Nzk2NzIzMzMwMDY0MzMiLCJlbWFpbCI6InRhbnFpbmd5dTJAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsIm5iZiI6MTY5ODMwOTcwOSwibmFtZSI6InFpbmd5dSB0YW4iLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EvQUNnOG9jSnRqSVdpcUNtWlJXRWRzVWFFa1ozRFU3OUlOQ3ZPOEg3bnYzSllTbFdxVnc9czk2LWMiLCJnaXZlbl9uYW1lIjoicWluZ3l1IiwiZmFtaWx5X25hbWUiOiJ0YW4iLCJsb2NhbGUiOiJ6aC1DTiIsImlhdCI6MTY5ODMxMDAwOSwiZXhwIjoxNjk4MzEzNjA5LCJqdGkiOiJiOGM1M2VjNjA2ZjE4OWIxYjRjNTFlZGIyZTU4ZGZmYWIwZjQxNTMyIn0.sfevZViBh8cGTGjvJt6KhPkCYb3krkiIfvWBue1GWzi50eo-C1AgdBzOetZaFmwb5TOkru07VAFESRFYyzW4cTw1TojGI8m6QUozgdVCOsnzyDmYUSlXxKXH4UkAfxInUDxuEeBy0VTcVOvSAFah6rGAsOFyb8foDa1pg3pdbrOplTAx23NgwT5GF640qSIi8J_8AHtPpseO7sklSVCBat0ibFV33IpJ6Wh3RtyPOC6ZQkzbKP6L-_PDeDJ7XSUOm_ziiBXPHFr0IbqGOjc06WlLjt53VqDt9YYkSRwopaZFVazMZdjqVwp33U9q87qsdC2GFhWBUfeOGk46-yB7tw)
![[Pasted image 20231026172356.png]]
## Âõõ„ÄÅÂèçÂêë‰º†Êí≠ÁÆóÊ≥ï

#### 1 Ê¢ØÂ∫¶‰∏ãÈôç
#### 2 ÈìæÂºèÊ±ÇÂØºÊ≥ïÂàô






